{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6449e487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import json\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "# Display settings: do not abbreviate DataFrame output\n",
    "pd.set_option(\"display.max_rows\", None)        # show all rows\n",
    "pd.set_option(\"display.max_columns\", None)     # show all columns\n",
    "pd.set_option(\"display.width\", None)           # don't wrap to fit console width\n",
    "pd.set_option(\"display.max_colwidth\", None)    # don't truncate column contents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ff296b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "responses/cancer/responses_obs5000_int200_shuf3_anon_gpt-4o-mini.csv: INCOMPLETE ()\n",
      "responses/cancer/responses_obs5000_int200_shuf3_anon_gpt-5-mini.csv: INCOMPLETE ()\n",
      "responses/cancer/responses_obs5000_int200_shuf3_anon_rules_gpt-4o-mini.csv: INCOMPLETE ()\n",
      "responses/cancer/responses_obs5000_int200_shuf3_anon_rules_gpt-5-mini.csv: INCOMPLETE ()\n",
      "responses/cancer/responses_obs5000_int200_shuf3_anon_rules_steps_gpt-4o-mini.csv: INCOMPLETE ()\n",
      "responses/cancer/responses_obs5000_int200_shuf3_anon_rules_steps_gpt-5-mini.csv: INCOMPLETE ()\n",
      "responses/cancer/responses_obs5000_int200_shuf3_anon_steps_gpt-4o-mini.csv: INCOMPLETE ()\n",
      "responses/cancer/responses_obs5000_int200_shuf3_anon_steps_gpt-5-mini.csv: INCOMPLETE ()\n",
      "responses/cancer/responses_obs5000_int200_shuf3_gpt-4o-mini.csv: INCOMPLETE ()\n",
      "responses/cancer/responses_obs5000_int200_shuf3_gpt-5-mini.csv: INCOMPLETE ()\n",
      "responses/cancer/responses_obs5000_int200_shuf3_rules_gpt-4o-mini.csv: INCOMPLETE ()\n",
      "responses/cancer/responses_obs5000_int200_shuf3_rules_gpt-5-mini.csv: INCOMPLETE ()\n",
      "responses/cancer/responses_obs5000_int200_shuf3_rules_steps_gpt-4o-mini.csv: INCOMPLETE ()\n",
      "responses/cancer/responses_obs5000_int200_shuf3_rules_steps_gpt-5-mini.csv: INCOMPLETE ()\n",
      "responses/cancer/responses_obs5000_int200_shuf3_steps_gpt-4o-mini.csv: INCOMPLETE ()\n",
      "responses/cancer/responses_obs5000_int200_shuf3_steps_gpt-5-mini.csv: INCOMPLETE ()\n",
      "\n",
      "=== Completeness Summary ===\n",
      "These files look incomplete:\n",
      "- responses/cancer/responses_obs5000_int200_shuf3_anon_gpt-4o-mini.csv: \n",
      "- responses/cancer/responses_obs5000_int200_shuf3_anon_gpt-5-mini.csv: \n",
      "- responses/cancer/responses_obs5000_int200_shuf3_anon_rules_gpt-4o-mini.csv: \n",
      "- responses/cancer/responses_obs5000_int200_shuf3_anon_rules_gpt-5-mini.csv: \n",
      "- responses/cancer/responses_obs5000_int200_shuf3_anon_rules_steps_gpt-4o-mini.csv: \n",
      "- responses/cancer/responses_obs5000_int200_shuf3_anon_rules_steps_gpt-5-mini.csv: \n",
      "- responses/cancer/responses_obs5000_int200_shuf3_anon_steps_gpt-4o-mini.csv: \n",
      "- responses/cancer/responses_obs5000_int200_shuf3_anon_steps_gpt-5-mini.csv: \n",
      "- responses/cancer/responses_obs5000_int200_shuf3_gpt-4o-mini.csv: \n",
      "- responses/cancer/responses_obs5000_int200_shuf3_gpt-5-mini.csv: \n",
      "- responses/cancer/responses_obs5000_int200_shuf3_rules_gpt-4o-mini.csv: \n",
      "- responses/cancer/responses_obs5000_int200_shuf3_rules_gpt-5-mini.csv: \n",
      "- responses/cancer/responses_obs5000_int200_shuf3_rules_steps_gpt-4o-mini.csv: \n",
      "- responses/cancer/responses_obs5000_int200_shuf3_rules_steps_gpt-5-mini.csv: \n",
      "- responses/cancer/responses_obs5000_int200_shuf3_steps_gpt-4o-mini.csv: \n",
      "- responses/cancer/responses_obs5000_int200_shuf3_steps_gpt-5-mini.csv: \n",
      "\n",
      "No complete files were successfully evaluated; no summary CSV written.\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = Path(\"responses/cancer\")\n",
    "SUMMARY_CSV = BASE_DIR / \"eval_summary.csv\"\n",
    "\n",
    "def parse_given_edges_tag(path: Path):\n",
    "    \"\"\"\n",
    "    Parse given-edges info from filenames like:\n",
    "      ..._gedge20_...\n",
    "    Returns (has_given_edges, given_edge_frac, given_edge_pct)\n",
    "      has_given_edges: 0/1\n",
    "      given_edge_frac: float or None\n",
    "      given_edge_pct:  int or None\n",
    "    \"\"\"\n",
    "    stem = path.stem  # e.g. responses_obs200_int3_shuf3_gedge20_anon_gpt-4o-mini\n",
    "    m = re.search(r\"_gedge(\\d+)\", stem)\n",
    "    if not m:\n",
    "        return 0, None, None\n",
    "    pct = int(m.group(1))\n",
    "    frac = pct / 100.0\n",
    "    return 1, frac, pct\n",
    "\n",
    "def count_nonempty(colname, rows):\n",
    "    return sum(1 for r in rows if (r.get(colname) or \"\").strip())\n",
    "\n",
    "def count_valid_flag(rows):\n",
    "    # valid column is expected to be 1/0 or truthy/falsy\n",
    "    return sum(1 for r in rows if str(r.get(\"valid\", \"\")).strip() in {\"1\", \"true\", \"True\"})\n",
    "\n",
    "def count_error_raw(rows):\n",
    "    # count rows where raw_response contains \"[ERROR]\"\n",
    "    return sum(\n",
    "        1\n",
    "        for r in rows\n",
    "        if \"[ERROR]\" in (r.get(\"raw_response\") or \"\")\n",
    "    )\n",
    "\n",
    "\n",
    "incomplete_files = []\n",
    "complete_files = []\n",
    "file_stats = {}  # file -> dict with n_rows, n_raw, n_valid, n_error\n",
    "\n",
    "\n",
    "if not BASE_DIR.exists():\n",
    "    print(f\"Base directory not found: {BASE_DIR.resolve()}\")\n",
    "else:\n",
    "    for csv_path in sorted(BASE_DIR.rglob(\"*.csv\")):\n",
    "        try:\n",
    "            with csv_path.open(\"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "                reader = csv.DictReader(f)\n",
    "                rows = list(reader)\n",
    "                fieldnames = reader.fieldnames or []\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to read {csv_path}: {e}\")\n",
    "            incomplete_files.append((csv_path, \"read_error\"))\n",
    "            continue\n",
    "\n",
    "        n_rows = len(rows)\n",
    "\n",
    "        # Detect ENCO vs LLM responses by filename\n",
    "        is_enco = \"ENCO\" in csv_path.name\n",
    "\n",
    "        # Which column (if any) is the \"prompt-like\" column? (for logging only)\n",
    "        if \"prompt\" in fieldnames:\n",
    "            prompt_col = \"prompt\"\n",
    "        elif \"prompt_path\" in fieldnames:\n",
    "            prompt_col = \"prompt_path\"\n",
    "        else:\n",
    "            prompt_col = None\n",
    "\n",
    "        has_raw = \"raw_response\" in fieldnames\n",
    "\n",
    "        if is_enco:\n",
    "            # ENCO files: no prompt/raw_response expected.\n",
    "            n_prompt = 0\n",
    "            n_raw = 0\n",
    "            n_valid = 0\n",
    "            n_error = 0\n",
    "\n",
    "            complete = n_rows > 0\n",
    "            status = \"OK\" if complete else \"INCOMPLETE\"\n",
    "            print(\n",
    "                f\"{csv_path} -> rows={n_rows}, (ENCO: skipping prompt/raw checks) \"\n",
    "                f\"valid_flags={n_valid} [{status}]\"\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # Normal LLM response files (must have raw_response)\n",
    "            if not has_raw:\n",
    "                # We cannot evaluate without raw_response column\n",
    "                n_prompt = count_nonempty(prompt_col, rows) if prompt_col else 0\n",
    "                n_raw = 0\n",
    "                n_valid = 0\n",
    "                n_error = 0\n",
    "\n",
    "                complete = False\n",
    "                status = \"INCOMPLETE\"\n",
    "                reason = \"missing raw_response column\"\n",
    "                print(f\"{csv_path} -> rows={n_rows}, prompts={n_prompt}, \"\n",
    "                      f\"raw_responses={n_raw}, error_responses={n_error}, \"\n",
    "                      f\"valid_flags={n_valid} [{status}] ({reason})\")\n",
    "\n",
    "                file_stats[csv_path] = {\n",
    "                    \"n_rows\": n_rows,\n",
    "                    \"completed_rows\": n_raw,\n",
    "                    \"valid_flag_rows\": n_valid,\n",
    "                    \"error_raw_responses\": n_error,\n",
    "                }\n",
    "                incomplete_files.append((csv_path, reason))\n",
    "                continue\n",
    "\n",
    "            # We *do* have raw_response; use it for completeness\n",
    "            n_prompt = count_nonempty(prompt_col, rows) if prompt_col else 0\n",
    "            n_raw = count_nonempty(\"raw_response\", rows)\n",
    "            n_valid = count_valid_flag(rows) if \"valid\" in fieldnames else 0\n",
    "            n_error = count_error_raw(rows)\n",
    "\n",
    "            # Completeness = every row has some raw_response\n",
    "            complete = (n_raw == n_rows)\n",
    "            status = \"OK\" if complete else \"INCOMPLETE\"\n",
    "\n",
    "            print(\n",
    "                f\"{csv_path} -> rows={n_rows}, prompts={n_prompt}, \"\n",
    "                f\"raw_responses={n_raw}, error_responses={n_error}, \"\n",
    "                f\"valid_flags={n_valid} [{status}]\"\n",
    "            )\n",
    "\n",
    "        # Store stats for later use in the summary CSV\n",
    "        file_stats[csv_path] = {\n",
    "            \"n_rows\": n_rows,\n",
    "            \"completed_rows\": n_raw,\n",
    "            \"valid_flag_rows\": n_valid,\n",
    "            \"error_raw_responses\": n_error,\n",
    "        }\n",
    "\n",
    "        if complete:\n",
    "            complete_files.append(csv_path)\n",
    "        else:\n",
    "            # Explicit reason matching our completeness rule\n",
    "            if is_enco:\n",
    "                reason = f\"rows={n_rows}, ENCO_file\"\n",
    "            else:\n",
    "                reason = (\n",
    "                    f\"rows={n_rows}, raw_responses={n_raw}, \"\n",
    "                    f\"has_raw={has_raw}\"\n",
    "                )\n",
    "            incomplete_files.append((csv_path, reason))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7309a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"experiments/responses/cancer/responses_obs5000_int200_shuf3_anon_steps_gpt-5-mini.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c625344",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'responses/cancer/eval_summary.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mresponses/cancer/eval_summary.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m mask = (\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# df['file'].str.contains('steps') &\u001b[39;00m\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# df['file'].str.contains('obs200_int3_shuf3_anon') &\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# & (df['valid'].astype(int) == 0)\u001b[39;00m\n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# df[mask][['file', 'num_rows','error_raw_responses','valid', 'avg_f1', 'avg_shd']].sort_values('avg_f1', ascending=False)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/enco/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/enco/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/enco/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/enco/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/enco/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'responses/cancer/eval_summary.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('responses/cancer/eval_summary.csv')\n",
    "mask = (\n",
    "    # df['file'].str.contains('steps') &\n",
    "    # df['file'].str.contains('obs200_int3_shuf3_anon') &\n",
    "    # df['file'].str.contains('responses') &\n",
    "    df['file'].str.contains('_anon_rules_steps_gpt', case=False)\n",
    "    # & ~df['file'].str.contains('anon')\n",
    "    # & (df['valid'].astype(int) == 0)\n",
    ")\n",
    "# df[mask][['file', 'num_rows','error_raw_responses','valid', 'avg_f1', 'avg_shd']].sort_values('avg_f1', ascending=False)\n",
    "df[mask].sort_values('avg_f1', ascending=False)[[\"file\",\"num_pred_edges\",\"avg_f1\",\"avg_shd\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03178394",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'responses/cancer/eval_summary.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m\n\u001b[32m      5\u001b[39m EVAL_SUMMARY = Path(\u001b[33m\"\u001b[39m\u001b[33mresponses/cancer/eval_summary.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEVAL_SUMMARY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m df = df[~df[\u001b[33m'\u001b[39m\u001b[33mfile\u001b[39m\u001b[33m'\u001b[39m].str.contains(\u001b[33m'\u001b[39m\u001b[33m4B\u001b[39m\u001b[33m'\u001b[39m, case=\u001b[38;5;28;01mFalse\u001b[39;00m) &\n\u001b[32m      9\u001b[39m         ~df[\u001b[33m'\u001b[39m\u001b[33mfile\u001b[39m\u001b[33m'\u001b[39m].str.contains(\u001b[33m'\u001b[39m\u001b[33m2024-07-18\u001b[39m\u001b[33m'\u001b[39m, case=\u001b[38;5;28;01mFalse\u001b[39;00m)]\n\u001b[32m     10\u001b[39m df = df[[df[\u001b[33m'\u001b[39m\u001b[33mfile\u001b[39m\u001b[33m'\u001b[39m].str.contains(\u001b[33m'\u001b[39m\u001b[33mint200\u001b[39m\u001b[33m'\u001b[39m) | df[\u001b[33m'\u001b[39m\u001b[33mfile\u001b[39m\u001b[33m'\u001b[39m].str.contains(\u001b[33m'\u001b[39m\u001b[33mobs5000\u001b[39m\u001b[33m'\u001b[39m)]]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/enco/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/enco/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/enco/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/enco/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/enco/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'responses/cancer/eval_summary.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "EVAL_SUMMARY = Path(\"responses/cancer/eval_summary.csv\")\n",
    "\n",
    "df = pd.read_csv(EVAL_SUMMARY)\n",
    "df = df[~df['file'].str.contains('4B', case=False) &\n",
    "        ~df['file'].str.contains('2024-07-18', case=False)]\n",
    "df = df[[df['file'].str.contains('int200') | df['file'].str.contains('obs5000')]]\n",
    "# --- 0. Keep only anon LLM runs + ENCO baseline --------------------\n",
    "file_str = df[\"file\"].astype(str)\n",
    "is_anon = file_str.str.contains(\"_anon\", regex=False)\n",
    "is_enco = file_str.str.contains(\"ENCO\", regex=False)  # non-anon ENCO runs\n",
    "df = df[is_anon | is_enco].reset_index(drop=True)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def detect_variant(path_str: str) -> str:\n",
    "    name = Path(path_str).name.lower()\n",
    "    if \"rules_steps\" in name:\n",
    "        return \"rules_steps\"\n",
    "    if \"rules\" in name:\n",
    "        return \"rules\"\n",
    "    if \"steps\" in name:\n",
    "        return \"steps\"\n",
    "    return \"base\"\n",
    "\n",
    "df[\"variant\"] = df[\"file\"].astype(str).apply(detect_variant)\n",
    "\n",
    "# Make life easier: ensure 'valid' exists & integer\n",
    "if \"valid\" in df.columns:\n",
    "    df[\"valid\"] = df[\"valid\"].fillna(0).astype(int)\n",
    "else:\n",
    "    df[\"valid\"] = 0\n",
    "\n",
    "\n",
    "# --- 2. Metric lookup helpers --------------------------------------\n",
    "\n",
    "def lookup_metrics(setting_tags, model_tag,\n",
    "                   variant=None,\n",
    "                   min_valid=1,\n",
    "                   given_edge_count=None):\n",
    "    \"\"\"\n",
    "    setting_tags: list like [\"obs5000_int200\"]\n",
    "    model_tag:    e.g. \"gpt-4o-mini\" or \"ENCO\"\n",
    "    variant:      one of {\"base\", \"rules\", \"steps\", \"rules_steps\"} or None\n",
    "    given_edge_count: if not None, require df['given_edge_count'] == this value\n",
    "    \"\"\"\n",
    "    m = df[\"file\"].astype(str).str.contains(setting_tags[0], regex=False)\n",
    "    for s in setting_tags[1:]:\n",
    "        m &= df[\"file\"].astype(str).str.contains(s, regex=False)\n",
    "\n",
    "    # model name constraint\n",
    "    m &= df[\"file\"].astype(str).str.contains(model_tag, regex=False)\n",
    "\n",
    "    # only apply valid filter if requested\n",
    "    if min_valid > 0:\n",
    "        m &= df[\"valid\"] >= min_valid\n",
    "\n",
    "    # variant constraint (base / rules / steps / rules_steps)\n",
    "    if variant is not None:\n",
    "        m &= (df[\"variant\"] == variant)\n",
    "\n",
    "    # given-edge constraint\n",
    "    if given_edge_count is not None and \"given_edge_count\" in df.columns:\n",
    "        m &= (df[\"given_edge_count\"] == given_edge_count)\n",
    "\n",
    "    subset = df[m]\n",
    "    if subset.empty:\n",
    "        return None, None\n",
    "\n",
    "    row = subset.iloc[0]   # or subset.mean() if you prefer\n",
    "    return row[\"avg_f1\"], row[\"avg_shd\"]\n",
    "\n",
    "\n",
    "def fmt(x, ndigits=2):\n",
    "    if x is None or (isinstance(x, float) and (math.isnan(x) or math.isinf(x))):\n",
    "        return \"\"\n",
    "    return f\"{x:.{ndigits}f}\"\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Settings (Obs/Inter combinations)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "SETTINGS = {\n",
    "    \"O5000_I3\": [\"obs5000_int200\"],\n",
    "    \"O5000_I0\": [\"obs5000_int0\"],\n",
    "    \"O0_I3\":   [\"obs0_int200\"],\n",
    "}\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. Extract model tags from filenames (LLMs only; ENCO is special)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def extract_model_tag(path_str: str) -> str:\n",
    "    \"\"\"Get the model part from a path, e.g.\n",
    "       '...responses_obs0_int200_shuf3_anon_Qwen3-32B.csv' -> 'Qwen3-32B'\n",
    "       '...predictions_obs5000_int200_ENCO.csv'             -> 'ENCO'\n",
    "    \"\"\"\n",
    "    stem = Path(path_str).stem    # e.g. 'responses_obs0_int200_shuf3_anon_Qwen3-32B'\n",
    "    parts = stem.split(\"_\")\n",
    "    return parts[-1]\n",
    "\n",
    "# Only look at response files for LLM models (ENCO is in predictions files)\n",
    "mask_llm = df[\"file\"].str.contains(\"responses\", regex=False) & \\\n",
    "           ~df[\"file\"].str.contains(\"ENCO\", regex=False)\n",
    "df_models = df[mask_llm].copy()\n",
    "df_models[\"model\"] = df_models[\"file\"].apply(extract_model_tag)\n",
    "\n",
    "all_models = sorted(df_models[\"model\"].unique())\n",
    "# print(\"Detected LLM models:\", all_models)\n",
    "\n",
    "llm_models = all_models  # ENCO handled separately\n",
    "\n",
    "PRETTY_NAME = {\n",
    "    \"gemini-2.5-flash\": \"Gemini-2.5-Flash\",\n",
    "    # add others if you care; fall back to tag itself\n",
    "}\n",
    "def pretty_model_name(model_tag: str) -> str:\n",
    "    return PRETTY_NAME.get(model_tag, model_tag)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5. Variant blocks (sections)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "VARIANTS = [\n",
    "    (\"base\",        r\"\\multicolumn{7}{l}{\\textbf{\\textit{Zero-Shot LLMs}}} \\\\\"),\n",
    "    (\"rules\",       r\"\\multicolumn{7}{l}{\\textbf{\\textit{Zero-Shot LLMs + Causality Rules}}} \\\\\"),\n",
    "    (\"steps\",       r\"\\multicolumn{7}{l}{\\textbf{\\textit{Zero-Shot LLMs + CD Steps}}} \\\\\"),\n",
    "    (\"rules_steps\", r\"\\multicolumn{7}{l}{\\textbf{\\textit{Zero-Shot LLMs + Causality Rules + CD Steps}}} \\\\\"),\n",
    "]\n",
    "\n",
    "ROWS = []\n",
    "\n",
    "# 5.a ENCO (pure causal discovery method, non-anon predictions_*.csv)\n",
    "ROWS.append({\n",
    "    \"section\": r\"\\multicolumn{7}{l}{\\textbf{\\textit{Pure Causal Discovery Method}}} \\\\\",\n",
    "    \"label\":   \"ENCO\",\n",
    "    \"model_tag\": \"ENCO\",\n",
    "    \"variant\": \"base\",\n",
    "    \"given_edge_count\": None,   # no given-edge prior\n",
    "})\n",
    "\n",
    "# 5.b LLM rows for each variant + model\n",
    "for variant_key, section_header in VARIANTS:\n",
    "    first_in_section = True\n",
    "    for model_tag in llm_models:\n",
    "        ROWS.append({\n",
    "            \"section\":   section_header if first_in_section else None,\n",
    "            \"label\":     pretty_model_name(model_tag),\n",
    "            \"model_tag\": model_tag,\n",
    "            \"variant\":   variant_key,\n",
    "            \"given_edge_count\": None,   # normal zero-shot / rules / steps runs\n",
    "        })\n",
    "        first_in_section = False\n",
    "\n",
    "# 5.c NEW: Zero-Shot LLMs + Given One Edge\n",
    "# pick models that have given_edge_count == 1 somewhere\n",
    "if \"given_edge_count\" in df.columns:\n",
    "    mask_given = (\n",
    "        (df[\"given_edge_count\"] == 1) &\n",
    "        df[\"file\"].str.contains(\"responses\", regex=False) &\n",
    "        ~df[\"file\"].str.contains(\"ENCO\", regex=False)\n",
    "    )\n",
    "    df_given = df[mask_given].copy()\n",
    "    given_models = sorted(df_given[\"file\"].apply(extract_model_tag).unique())\n",
    "else:\n",
    "    given_models = []\n",
    "\n",
    "first_in_section = True\n",
    "for model_tag in given_models:\n",
    "    ROWS.append({\n",
    "        \"section\": r\"\\multicolumn{7}{l}{\\textbf{\\textit{Zero-Shot LLMs + Given One Edge}}} \\\\\"\n",
    "                   if first_in_section else None,\n",
    "        \"label\":   pretty_model_name(model_tag),\n",
    "        \"model_tag\": model_tag,\n",
    "        \"variant\": \"base\",          # assume these are base + given-edge\n",
    "        \"given_edge_count\": 1,      # key for filtering in lookup_metrics\n",
    "    })\n",
    "    first_in_section = False\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 6. Build LaTeX lines (F1 / SHD table)\n",
    "# -------------------------------------------------------------------\n",
    "lines = []\n",
    "\n",
    "lines.append(r\"\\begin{table*}[ht!]\")\n",
    "lines.append(r\"\\centering\")\n",
    "lines.append(r\"\\setlength{\\tabcolsep}{6pt}\")\n",
    "lines.append(r\"\\caption{Causal discovery performance on the \\textit{Cancer} causal graph. The variable names are anonymized.}\")\n",
    "lines.append(r\"\\resizebox{\\textwidth}{!}{%\")\n",
    "lines.append(r\"\\begin{tabular}{lcccccc}\")\n",
    "lines.append(r\"\\toprule\")\n",
    "lines.append(\n",
    "    r\" & \\multicolumn{2}{c}{\\textbf{Obs. 5000, Inter. 200}}\"\n",
    "    r\" & \\multicolumn{2}{c}{\\textbf{Obs. 5000, Inter. 0}}\"\n",
    "    r\" & \\multicolumn{2}{c}{\\textbf{Obs. 0, Inter. 200}} \\\\\"\n",
    ")\n",
    "lines.append(r\"\\cmidrule(lr){2-3}\\cmidrule(lr){4-5}\\cmidrule(lr){6-7}\")\n",
    "lines.append(\n",
    "    r\"\\textbf{Method} & \\textbf{SHD}~$\\downarrow$ & \\textbf{F1}~$\\uparrow$\"\n",
    "    r\" & \\textbf{SHD}~$\\downarrow$ & \\textbf{F1}~$\\uparrow$\"\n",
    "    r\" & \\textbf{SHD}~$\\downarrow$ & \\textbf{F1}~$\\uparrow$ \\\\\"\n",
    ")\n",
    "\n",
    "lines.append(r\"\\midrule\")\n",
    "\n",
    "for rowdef in ROWS:\n",
    "    # Optional section header\n",
    "    if rowdef.get(\"section\"):\n",
    "        lines.append(rowdef[\"section\"])\n",
    "\n",
    "    label       = rowdef[\"label\"]\n",
    "    model_tag   = rowdef[\"model_tag\"]\n",
    "    variant_key = rowdef[\"variant\"]\n",
    "    given_edge_count = rowdef.get(\"given_edge_count\", None)\n",
    "\n",
    "    # ENCO: don't require valid>=1, and ignore variant filter\n",
    "    is_enco = (model_tag == \"ENCO\")\n",
    "    min_valid = 0 if is_enco else 1\n",
    "    variant_for_lookup = None if is_enco else variant_key\n",
    "\n",
    "    cells = []\n",
    "    for key, setting_subs in SETTINGS.items():\n",
    "        f1, shd = lookup_metrics(\n",
    "            setting_subs,\n",
    "            model_tag=model_tag,\n",
    "            variant=variant_for_lookup,\n",
    "            min_valid=min_valid,\n",
    "            given_edge_count=given_edge_count,\n",
    "        )\n",
    "        cells.append(fmt(f1))\n",
    "        cells.append(fmt(shd, ndigits=2))\n",
    "\n",
    "    # If *all* entries for this row are empty, skip the row entirely\n",
    "    if all(c == \"\" for c in cells):\n",
    "        continue\n",
    "\n",
    "    row_tex = (\n",
    "        f\"{label} & {cells[1]} & {cells[0]} & \"\n",
    "        f\"{cells[3]} & {cells[2]} & {cells[5]} & {cells[4]} \\\\\\\\\"\n",
    "    )\n",
    "\n",
    "    lines.append(row_tex)\n",
    "    lines.append(r\"\\addlinespace[0.8ex]\")\n",
    "\n",
    "lines.append(r\"\\bottomrule\")\n",
    "lines.append(r\"\\end{tabular}\")\n",
    "lines.append(r\"}\")\n",
    "lines.append(r\"\\end{table*}\")\n",
    "\n",
    "latex_table = \"\\n\".join(lines)\n",
    "print(latex_table)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 7. Valid-ratio table, same sections (including Given One Edge)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def lookup_valid_counts(setting_tags, model_tag,\n",
    "                        variant=None,\n",
    "                        min_rows=1,\n",
    "                        given_edge_count=None):\n",
    "    \"\"\"\n",
    "    Returns (valid, num_rows) or (None, None) if not found.\n",
    "    \"\"\"\n",
    "    m = df[\"file\"].astype(str).str.contains(setting_tags[0], regex=False)\n",
    "    for s in setting_tags[1:]:\n",
    "        m &= df[\"file\"].astype(str).str.contains(s, regex=False)\n",
    "\n",
    "    # constrain to this model\n",
    "    m &= df[\"file\"].astype(str).str.contains(model_tag, regex=False)\n",
    "\n",
    "    # constrain to variant if given\n",
    "    if variant is not None:\n",
    "        m &= (df[\"variant\"] == variant)\n",
    "\n",
    "    # constrain to given_edge_count if given\n",
    "    if given_edge_count is not None and \"given_edge_count\" in df.columns:\n",
    "        m &= (df[\"given_edge_count\"] == given_edge_count)\n",
    "\n",
    "    subset = df[m]\n",
    "    if subset.empty:\n",
    "        return None, None\n",
    "\n",
    "    row = subset.iloc[0]\n",
    "    num_rows = row.get(\"num_rows\", 0)\n",
    "    valid    = row.get(\"valid\", 0)\n",
    "\n",
    "    if not num_rows or num_rows < min_rows:\n",
    "        return None, None\n",
    "\n",
    "    return int(valid), int(num_rows)\n",
    "\n",
    "\n",
    "valid_lines = []\n",
    "\n",
    "valid_lines.append(r\"\\begin{table*}[ht!]\")\n",
    "valid_lines.append(r\"\\centering\")\n",
    "valid_lines.append(r\"\\setlength{\\tabcolsep}{6pt}\")\n",
    "valid_lines.append(\n",
    "    r\"\\caption{Valid adjacency extraction ratio for each method and data setting on the \\textit{Cancer} graph.}\"\n",
    ")\n",
    "valid_lines.append(r\"\\resizebox{\\textwidth}{!}{%\")\n",
    "valid_lines.append(r\"\\begin{tabular}{lccc}\")\n",
    "valid_lines.append(r\"\\toprule\")\n",
    "valid_lines.append(\n",
    "    r\" & \\textbf{Obs. 5000, Inter. 200}\"\n",
    "    r\" & \\textbf{Obs. 5000, Inter. 0}\"\n",
    "    r\" & \\textbf{Obs. 0, Inter. 200} \\\\\"\n",
    ")\n",
    "valid_lines.append(r\"\\midrule\")\n",
    "\n",
    "for rowdef in ROWS:\n",
    "    # Optional section header â€“ adapt from 7 columns to 4\n",
    "    if rowdef.get(\"section\"):\n",
    "        sec = rowdef[\"section\"]\n",
    "        sec = sec.replace(r\"\\multicolumn{7}\", r\"\\multicolumn{4}\")\n",
    "        valid_lines.append(sec)\n",
    "\n",
    "    label       = rowdef[\"label\"]\n",
    "    model_tag   = rowdef[\"model_tag\"]\n",
    "    variant_key = rowdef[\"variant\"]\n",
    "    given_edge_count = rowdef.get(\"given_edge_count\", None)\n",
    "\n",
    "    cells = []\n",
    "    for key, setting_subs in SETTINGS.items():\n",
    "        valid_count, num_rows = lookup_valid_counts(\n",
    "            setting_subs,\n",
    "            model_tag=model_tag,\n",
    "            variant=variant_key if model_tag != \"ENCO\" else None,\n",
    "            given_edge_count=given_edge_count,\n",
    "        )\n",
    "\n",
    "        if valid_count is None or num_rows is None:\n",
    "            cells.append(\"\")  # empty cell\n",
    "        else:\n",
    "            cells.append(rf\"$\\frac{{{valid_count}}}{{{num_rows}}}$\")\n",
    "\n",
    "    # Skip row entirely if ALL settings are empty\n",
    "    if all(c == \"\" for c in cells):\n",
    "        continue\n",
    "\n",
    "    row_tex = f\"{label} & {cells[0]} & {cells[1]} & {cells[2]} \\\\\\\\\"\n",
    "    valid_lines.append(row_tex)\n",
    "    valid_lines.append(r\"\\addlinespace[0.8ex]\")\n",
    "\n",
    "valid_lines.append(r\"\\bottomrule\")\n",
    "valid_lines.append(r\"\\end{tabular}\")\n",
    "valid_lines.append(r\"}\")\n",
    "valid_lines.append(r\"\\end{table*}\")\n",
    "\n",
    "latex_valid_table = \"\\n\".join(valid_lines)\n",
    "print(latex_valid_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e648f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_pydot import graphviz_layout\n",
    "sys.path.append(\"../\")\n",
    "from causal_graphs.graph_real_world import load_graph_file\n",
    "from causal_graphs.graph_visualization import visualize_graph\n",
    "import matplotlib.pyplot as plt\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. Load the cancer graph\n",
    "# ---------------------------------------------------------------------\n",
    "bif_path = Path(\"../causal_graphs/real_data/small_graphs/cancer.bif\")\n",
    "graph = load_graph_file(str(bif_path))\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. Build a NetworkX graph with integer nodes (0..n-1)\n",
    "#    so we can reuse the same layout but change labels.\n",
    "# ---------------------------------------------------------------------\n",
    "G = nx.DiGraph()\n",
    "n = len(graph.variables)\n",
    "\n",
    "# Nodes are integers, but we keep two separate label dicts:\n",
    "labels_orig = {}\n",
    "labels_anon = {}\n",
    "\n",
    "for i, v in enumerate(graph.variables):\n",
    "    G.add_node(i)\n",
    "    labels_orig[i] = v.name        # original variable name\n",
    "    labels_anon[i] = f\"X{i+1}\"     # anonymized name\n",
    "\n",
    "# Add directed edges from the CausalDAG\n",
    "for (u_idx, v_idx) in graph.edges.tolist():\n",
    "    G.add_edge(u_idx, v_idx)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3. Compute a single layout (Graphviz) and reuse for both panels\n",
    "# ---------------------------------------------------------------------\n",
    "pos = graphviz_layout(G, prog=\"dot\")  # requires graphviz + pydot\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4. Plot side-by-side: original (left) and anonymized (right)\n",
    "# ---------------------------------------------------------------------\n",
    "figsize = max(3, n ** 0.7)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(2 * figsize, figsize))\n",
    "\n",
    "# Common draw kwargs to mimic your existing style\n",
    "draw_kwargs = dict(\n",
    "    arrows=True,\n",
    "    node_color=\"lightgrey\",\n",
    "    edgecolors=\"black\",\n",
    "    node_size=600,\n",
    "    arrowstyle=\"-|>\",\n",
    "    arrowsize=16,\n",
    ")\n",
    "\n",
    "# Left: original names\n",
    "ax = axes[1]\n",
    "nx.draw(G, pos, ax=ax, with_labels=False, **draw_kwargs)\n",
    "nx.draw_networkx_labels(G, pos, labels=labels_orig, font_weight=\"bold\", ax=ax)\n",
    "ax.set_title(\"Cancer graph (original names)\")\n",
    "ax.set_axis_off()\n",
    "ax.margins(0.2)  # <<< add padding inside axes\n",
    "# Right: anonymized names\n",
    "ax = axes[0]\n",
    "nx.draw(G, pos, ax=ax, with_labels=False, **draw_kwargs)\n",
    "nx.draw_networkx_labels(G, pos, labels=labels_anon, font_weight=\"bold\", ax=ax)\n",
    "ax.set_title(\"Cancer graph (anonymized)\")\n",
    "ax.set_axis_off()\n",
    "ax.margins(0.2)\n",
    "plt.tight_layout(pad=1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"cancer_graph_original_vs_anon.pdf\", bbox_inches=\"tight\", transparent=True)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a571db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = pd.read_csv(\"responses/cancer/responses_obs200_int3_shuf3_anon_Qwen3-32B.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea8e8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_check['prediction'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a544c37d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
